{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing and stochastic modelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numpy\n",
    "import numpy as np\n",
    "#Scipy\n",
    "from scipy import special as scsp\n",
    "from scipy import stats\n",
    "from numpy import random as rdm\n",
    "#Pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "#Multi-CPU job\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing as mpg\n",
    "#Time\n",
    "import time\n",
    "#Json\n",
    "import json\n",
    "#Os for file management\n",
    "import os.path\n",
    "\n",
    "#Home made functions\n",
    "import Utilities\n",
    "\n",
    "#####Reference values and ranges for filename generation#####\n",
    "UREF = 4.6E-06\n",
    "TREF = 2.92E+01\n",
    "RERUN = True #To rerun all computations (False will load from file if they exist)\n",
    "NCORES_MAX = -2 #Maximal number of cores in use\n",
    "extension = '.pdf' #fig file extension\n",
    "DIR  = 'D:/pdm_data/' #On desktop\n",
    "#DIR  = 'C:/Users/Antoine/Documents/Etudes/Master CSE/PDM/data/' #On laptop\n",
    "FDIR = DIR + 'Figures_PeInf'#'../report/Figures/' #Figure directory\n",
    "ODIR = DIR + '/Processed_data_PeInf/' #Output directory\n",
    "CASE   = 'Bentheimer1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skew-normal PDF\n",
    "def p_skew(v,sample) :\n",
    "    alpha = sample['alpha']\n",
    "    mu    = sample['mu']\n",
    "    sigma = sample['sigma']\n",
    "    p = 1/np.sqrt(2.0*np.pi*sigma**2)*np.exp(-(v-mu)**2/(2.0*sigma**2))*scsp.erfc(-alpha*(v-mu)/np.sqrt(2.0*sigma**2))\n",
    "    return p\n",
    "\n",
    "def Save_data(MP_dict, filename) :\n",
    "    #Copy data into non np array dict for json formating\n",
    "    No_np = {}\n",
    "    for key in MP_dict : No_np[key] = [v for v in MP_dict[key]]\n",
    "    with open(filename, 'w') as outfile:  \n",
    "        json.dump(No_np, outfile)\n",
    "    \n",
    "def Load_data(filename) :\n",
    "    MP_dict = {}\n",
    "    with open(filename, 'r') as outfile:  \n",
    "        MP_dict = json.load(outfile)\n",
    "    #Converting arrays in np array\n",
    "    for key in MP_dict : MP_dict[key] = np.array(MP_dict[key])\n",
    "    return MP_dict\n",
    "\n",
    "def Extract_param (fname, param) :\n",
    "    p_dict = {}\n",
    "    for p_ in param :\n",
    "        k  = 0\n",
    "        for i in range(len(DIR),len(fname)) :\n",
    "            if fname[i:i+len(p_)] == p_ :\n",
    "                k = i+len(p_)\n",
    "                break\n",
    "        v_ = ''\n",
    "        while (fname[k] != '_' and fname[k:k+2] != '.d') :\n",
    "            v_ += fname[k]\n",
    "            k  += 1\n",
    "        if v_ == 'Inf' : v_ = 0\n",
    "        p_dict[p_] = float(v_)\n",
    "    return p_dict\n",
    "\n",
    "def time_step_analysis(t_ssl,key) :\n",
    "    dt         = [t1-t0 for (t1,t0) in zip(t_ssl[1:],t_ssl[:-1]) if t1-t0 > 0]\n",
    "    dt_hist    = Extract_pdf(dt,50)\n",
    "    dtDNS_mean = np.mean(dt)\n",
    "    dtDNS_std  = np.std(dt)\n",
    "    return (key, dt_hist, dtDNS_mean, dtDNS_std)\n",
    "    \n",
    "def parallel_time_step_analysis(t_dict) :\n",
    "    dt_hist        = {}\n",
    "    dtDNS_mean     = {}\n",
    "    dtDNS_std      = {}\n",
    "    results  = Parallel(n_jobs=NCORES_MAX)(delayed(time_step_analysis)(t_dict[key],key) for key in KEYS)\n",
    "    for (k_,h_,m_,s_) in results :\n",
    "        dt_hist[k_]    = h_.tolist()\n",
    "        dtDNS_mean[k_] = [m_]\n",
    "        dtDNS_std[k_]  = [s_]\n",
    "    return dt_hist, dtDNS_mean, dtDNS_std\n",
    "\n",
    "## Some useful functions to handle pdf\n",
    "def Save_pdf(pdf_dict, filename) :\n",
    "    #Copy data into non np array dict for json formating\n",
    "    x = {}\n",
    "    y = {}\n",
    "    for key in pdf_dict : x[key] = [v for v in pdf_dict[key][0]]\n",
    "    for key in pdf_dict : y[key] = [v for v in pdf_dict[key][1]]\n",
    "    with open(filename, 'w') as outfile:  \n",
    "        json.dump([x,y], outfile)\n",
    "        \n",
    "def Load_pdf(filename) :\n",
    "    x = {}\n",
    "    y = {} \n",
    "    pdf_ = {}\n",
    "    with open(filename, 'r') as outfile:  \n",
    "        data = json.load(outfile)\n",
    "    x = data[0]\n",
    "    y = data[1]\n",
    "    for key in x : pdf_[key] = [np.array(x[key]),np.array(y[key])]    \n",
    "    return pdf_\n",
    "\n",
    "def Extract_pdf(data_, nbins = 500, dst = True) :\n",
    "    #data_ = data_[~np.isnan(data_)]\n",
    "    pdf_ = np.histogram(data_, bins = nbins, density = dst);\n",
    "    pdf_ = np.array([pdf_[1][0:-1], pdf_[0]])\n",
    "    return pdf_\n",
    "\n",
    "def Return_mean(pdf_) :\n",
    "    x = pdf_[0]; y = pdf_[1];\n",
    "    return np.sum(x*y)/np.sum(y)\n",
    "\n",
    "def Return_std(pdf_) :\n",
    "    x    = pdf_[0]; y = pdf_[1];\n",
    "    mu = Return_mean(pdf_)\n",
    "    std  = np.sqrt(np.sum([(x_ - mu)**2*y_ for (x_,y_) in zip(x,y)])/np.sum(y))\n",
    "    return std\n",
    "\n",
    "def Return_skew(pdf_) :\n",
    "    x    = pdf_[0]; y = pdf_[1];\n",
    "    mu   = Return_mean(pdf_)\n",
    "    std  = Return_std(pdf_)\n",
    "    skew = np.sum([((x_ - mu)/std)**3*y_ for (x_,y_) in zip(x,y)])/np.sum(y)\n",
    "    return skew\n",
    "\n",
    "def mean_crossing_time(tdata) :\n",
    "    T_  = []\n",
    "    tm1 = 0\n",
    "    for t_ in tdata :\n",
    "        if tm1 > t_ : #If time is set to zero again (= end of the stream line)\n",
    "            T_.append(tm1)\n",
    "        tm1 = t_\n",
    "    N_  = len(T_)\n",
    "    mean_ = sum(T_)/N_\n",
    "    return mean_\n",
    "\n",
    "def v_to_s(v) :\n",
    "    if v == 0 :\n",
    "        return 'Inf'\n",
    "    else :\n",
    "        return '%.1E' % v\n",
    "\n",
    "def v_to_d(v) :\n",
    "    return '%.2f' % v\n",
    "    \n",
    "def v_to_pow(v) :\n",
    "    if v == 0 :\n",
    "        return r'$\\infty$'\n",
    "    else :\n",
    "        power  = int(np.log10(v))\n",
    "        factor = int(v/10.0**power)\n",
    "        if factor == 1.0 :\n",
    "            return r'$10^{'+str(power)+'}$'\n",
    "        else :\n",
    "            return str(factor)+r'$\\times 10^{'+str(power)+'}$'\n",
    "\n",
    "def print_keys(dict_) :\n",
    "    for key in dict_ : print(key)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bentheimer 1000 parameters\n",
    "Bentheimer1000 ={\n",
    "'name'  :  'Bentheimer 1000',\n",
    "'L0'    :  3.0,      #Sample length [mm]\n",
    "'Deltax':  3.0,      #Resolution [um]\n",
    "'n'     :  1000,     #Nb of 1D points\n",
    "'L0/L'  :  22.6,     #Pore length ratio [-]\n",
    "'phi'   :  0.22,     #Porosity [%]\n",
    "'kappa' :  1.65,     #Tortuosity\n",
    "'lmbd/L':  8.32,     #Correlation length [-]\n",
    "#Theta angle model\n",
    "'omegat':  4.3,\n",
    "'taut'  :  0.12,\n",
    "'sigmat':  0.69,\n",
    "'mut'   :  38.0,\n",
    "#Beta angle model\n",
    "'taub'  :  0.13,\n",
    "'bb'    :  1.9,\n",
    "#Log velocity magnitude model\n",
    "'b'     :  0.57,\n",
    "'c'     :  1.3,\n",
    "'alpha' : -3.6,\n",
    "'mu'    :  1.5,\n",
    "'sigma' :  2.5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset to load : 5\n"
     ]
    }
   ],
   "source": [
    "DATA = {}\n",
    "\n",
    "SAMPLE = 'Bentheimer 1000'\n",
    "PECLET = [0, 0, 0, 0, 0]\n",
    "SLSM   = [20000, 20000, 20000, 20000, 20000]\n",
    "CSF    = [1, 1, 1, 1, 1]\n",
    "IDX    = [1, 2, 3, 4, 5]\n",
    "KEYS   = []\n",
    "for (Pe_, slsm_, csf_, idx_) in zip(PECLET, SLSM, CSF, IDX) : \n",
    "    name_ = DIR+CASE+'/Particle_Tracking/sl_out_Pe'+v_to_s(Pe_)+'_SLSM'+v_to_s(slsm_)+'_TMAX0_CSF'+str(csf_)+'_'+str(idx_)+'.dat'\n",
    "    key_  = 'Pe='+v_to_pow(Pe_)+', SLSM='+v_to_pow(slsm_)+', CSF='+str(csf_) +', #'+str(idx_)\n",
    "    DATA[key_] = name_\n",
    "    KEYS.append(key_)\n",
    "\n",
    "PLOT_KEYS = KEYS\n",
    "\n",
    "color_cycle = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "print('Dataset to load : ' +str(len(DATA)))\n",
    "\n",
    "PARAM = {}\n",
    "param = ['Pe','SLSM','TMAX','CSF']\n",
    "for key in DATA : PARAM[key] = Extract_param (DATA[key], param)\n",
    "    \n",
    "for key in DATA :\n",
    "    if not os.path.isfile(DATA[key]) :\n",
    "        print('Warning file from key : ' + key + ' :')\n",
    "        print('\\t' + DATA[key])\n",
    "        print('not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and processing DNS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing Ux mean from Eulerian velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Umean fortran = [2.4514361e-09]\t Umean dmc = [7.17953958e-09]\n"
     ]
    }
   ],
   "source": [
    "UMEAN = {}\n",
    "Ndat  = 1000\n",
    "if (not os.path.isfile(DIR+'Processed_data/UMEAN'+str(Ndat)+'.dat')) :\n",
    "    phi   = Bentheimer1000['phi']\n",
    "    print('-load ux data..')\n",
    "    ux    = np.loadtxt(DIR+'Bentheimer'+str(Ndat)+'/Ux.dat')\n",
    "    print('-Ux dimensions : '+str(np.shape(ux)))\n",
    " \n",
    "    #First method to find Umean (direct mesh computation)\n",
    "    print('-first method..')\n",
    "    Umean = np.mean(ux[:][0]) #Mean flow velocity as the mean over the first yz section of the domain\n",
    "    UMEAN['dmc'] = [Umean]\n",
    "\n",
    "    #Second method (From Fortran code)\n",
    "    print('-reshaping ux..')\n",
    "    uux = ux.reshape(Ndat**3)\n",
    "    del ux\n",
    "    Umean = 0\n",
    "    print('-second method..')\n",
    "    for (i2,i3) in zip(range(Ndat),range(Ndat)) :\n",
    "        i1 = 0\n",
    "        Umean += uux[Ndat*Ndat*i3 + Ndat*i2 + i1]\n",
    "    Umean = Umean/(Ndat**2)/phi\n",
    "    UMEAN['fortran'] = [Umean]\n",
    "    Save_data(UMEAN,ODIR+'UMEAN'+str(Ndat)+'.dat')\n",
    "    del uux\n",
    "else :\n",
    "    UMEAN = Load_data(DIR+'Processed_data/UMEAN'+str(Ndat)+'.dat')\n",
    "    \n",
    "print('=> Umean fortran = ' + str(UMEAN['fortran']) + '\\t Umean dmc = ' + str(UMEAN['dmc']))\n",
    "Umean = UMEAN['fortran'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing log velocity magnitude $$v = \\ln{\\left(\\frac{\\sqrt{u_x^2 + u_y^2 + u_z^2}}{U}\\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vDNS = {}; \n",
    "vfname    = ODIR+'vDNS.dat'\n",
    "if os.path.isfile(vfname)    and (not RERUN) : \n",
    "    print('Loading vDNS..')\n",
    "    vDNS     = Load_data(vfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDNS = {};    \n",
    "tfname    = ODIR+'tDNS.dat'\n",
    "if os.path.isfile(tfname)    and (not RERUN) : \n",
    "    print('Loading tDNS..')\n",
    "    tDNS     = Load_data(tfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlDNS = {};\n",
    "dlfname   = ODIR+'dlDNS.dat'\n",
    "if os.path.isfile(dlfname)   and (not RERUN) : \n",
    "    print('Loading dlDNS..')\n",
    "    dlDNS    = Load_data(dlfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaDNS = {};\n",
    "thetafname= ODIR+'thetaDNS.dat'\n",
    "if os.path.isfile(thetafname)and (not RERUN) :\n",
    "    print('Loading thetaDNS..')\n",
    "    thetaDNS = Load_data(thetafname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvx_DNS = {}; pdfvxfname = ODIR+'pdf_vx.dat'\n",
    "pvy_DNS = {}; pdfvyfname = ODIR+'pdf_vy.dat'\n",
    "pvz_DNS = {}; pdfvzfname = ODIR+'pdf_vz.dat'\n",
    "if os.path.isfile(pdfvxfname) and (not RERUN):  pvx_DNS = Load_pdf(pdfvxfname)\n",
    "if os.path.isfile(pdfvyfname) and (not RERUN):  pvy_DNS = Load_pdf(pdfvyfname)\n",
    "if os.path.isfile(pdfvzfname) and (not RERUN):  pvz_DNS = Load_pdf(pdfvzfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if new data file...\n",
      "Pe=$\\infty$, SLSM=2$\\times 10^{4}$, CSF=1, #1 not found..\n",
      "Opening dataDNS..\n",
      "Time needed : 1404.5106747150421\n",
      "Pe=$\\infty$, SLSM=2$\\times 10^{4}$, CSF=1, #1\t: NtDNS = 59975598\n",
      "missing Pe=$\\infty$, SLSM=2$\\times 10^{4}$, CSF=1, #1 in LVM ..\n"
     ]
    }
   ],
   "source": [
    "print('Checking if new data file...')\n",
    "U = np.array([1,0,0])\n",
    "SAVE_LVM   = False\n",
    "SAVE_DLDNS = False\n",
    "SAVE_THETA = False\n",
    "SAVE_PDF   = False\n",
    "for key in KEYS :\n",
    "    if (key not in vDNS) or (key not in pvx_DNS) or (key not in dlDNS) or (key not in thetaDNS and key == KEYS[0]) or RERUN :\n",
    "        print(key+' not found..')\n",
    "        RESAVE = True\n",
    "        print('Opening dataDNS..')\n",
    "        start = time.time()\n",
    "        with open(DATA[key], 'r') as f:\n",
    "            dataDNS = np.loadtxt((line for line in f if (len(line)>100 and len(line)<150)))\n",
    "        tDNS[key] = dataDNS[:,0]\n",
    "        print('Time needed : ' + str(time.time()-start))\n",
    "        print(key + '\\t: NtDNS = '+str(len(tDNS[key])))\n",
    "            \n",
    "        ## LVM\n",
    "        if (key not in vDNS) :\n",
    "            print(\"missing \" + key + \" in LVM ..\")\n",
    "            SAVE_LVM = True\n",
    "            vDNS[key] = np.array(\n",
    "                [np.log(np.sqrt(ux**2+uy**2+uz**2)) \n",
    "                    for (ux,uy,uz) in zip(dataDNS[:,4], dataDNS[:,5], dataDNS[:,6]) \n",
    "                    if np.sqrt(ux**2+uy**2+uz**2) > 0])  \n",
    "            #vDNS[key] = np.array(np.log(np.sqrt(dataDNS[:,4]**2+dataDNS[:,5]**2+dataDNS[:,6]**2))\n",
    "        \n",
    "        ## Traveled distance dl\n",
    "        if (key not in dlDNS) :\n",
    "            print(\"missing \" + key + \" in dlDNS ..\")\n",
    "            SAVE_DLDNS = True\n",
    "            dlDNS[key] = np.sqrt((dataDNS[1:,1]-dataDNS[:-1,1])**2 +\n",
    "                                 (dataDNS[1:,2]-dataDNS[:-1,2])**2 +\n",
    "                                 (dataDNS[1:,3]-dataDNS[:-1,3])**2)\n",
    "            #dlDNS[key] = np.abs(np.sqrt(dataDNS[1:,1]**2+dataDNS[1:,2]**2+dataDNS[1:,3]**2) -  #Old version\n",
    "            #                    np.sqrt(dataDNS[:-1,1]**2+dataDNS[:-1,2]**2+dataDNS[:-1,3]**2))        \n",
    "        ## Signed theta angle\n",
    "        if (KEYS[0] not in thetaDNS) : # only Pe Inf for the moment...\n",
    "            SAVE_THETA = True\n",
    "            print('Computing theta')\n",
    "            start = time.time()\n",
    "            thetaDNS[key] = np.zeros(len(dataDNS[:,0]))\n",
    "            u0 = dataDNS[0,4:7]\n",
    "            thetaDNS[key][0] = np.arccos(u0[0]/np.sqrt(np.dot(u0,u0)))\n",
    "            sgn  = +1\n",
    "            for i in range(1,len(dataDNS[:,0])) :\n",
    "                u1 = dataDNS[i,4:7] #actual velocity\n",
    "                sgn *= 1-2*(np.dot(u1[1:3],u0[1:3])<0) #if not on the same plane side, sgn changes sign\n",
    "                thetaDNS[key][i] = sgn * np.arccos(u1[0]/np.sqrt(np.dot(u1,u1))) #theta\n",
    "                u0   = u1\n",
    "        \n",
    "        ## DIRECTIONAL PDFs\n",
    "        if (key not in pvx_DNS) :\n",
    "            print(\"missing \" + key + \" in PDF ..\")\n",
    "            SAVE_PDF = True\n",
    "            pvx_DNS[key] = Extract_pdf(dataDNS[:,4])\n",
    "            pvy_DNS[key] = Extract_pdf(dataDNS[:,5])\n",
    "            pvz_DNS[key] = Extract_pdf(dataDNS[:,6])\n",
    "        \n",
    "        #free memspace\n",
    "        dataDNS = []\n",
    "        \n",
    "if SAVE_LVM :\n",
    "    print('Saving LVM...')\n",
    "    Save_data(vDNS,vfname)\n",
    "    Save_data(tDNS,tfname)\n",
    "if SAVE_DLDNS :\n",
    "    print('Saving dlDNS...')\n",
    "    Save_data(dlDNS,dlfname)\n",
    "if SAVE_THETA :\n",
    "    print('Saving theta...')\n",
    "    Save_data(thetaDNS,thetafname)      \n",
    "if SAVE_PDF :\n",
    "    print('Saving PDF..')\n",
    "    Save_pdf(pvx_DNS,pdfvxfname)\n",
    "    Save_pdf(pvy_DNS,pdfvyfname)\n",
    "    Save_pdf(pvz_DNS,pdfvzfname)\n",
    "print('Done.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of the rough DNS time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1, figsize=(5,4), sharey = True)\n",
    "ax = axs\n",
    "ax.set_title('Evolution of time with steps');\n",
    "for key in PLOT_KEYS : ax.plot(tDNS[key][range(100000)],'.--', label = key);\n",
    "ax.set_xlabel('step');\n",
    "ax.set_ylabel('t');\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossing_time = {}\n",
    "crossing_step = {}\n",
    "for key in tDNS :\n",
    "    crossing_time[key] = []\n",
    "    crossing_step[key] = []\n",
    "    t0  = -100\n",
    "    cnt = 0\n",
    "    for t1 in tDNS[key] :\n",
    "        if t1 < t0 :\n",
    "            crossing_time[key].append(t0)\n",
    "            crossing_step[key].append(cnt)\n",
    "            t0  = -1\n",
    "            cnt = 0\n",
    "        else :\n",
    "            t0 = t1\n",
    "            cnt += 1\n",
    "    crossing_time[key] = np.array(crossing_time[key])\n",
    "    crossing_step[key] = np.array(crossing_step[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossing_time_pdf = {key : Extract_pdf(crossing_time[key],50) for key in KEYS}\n",
    "crossing_step_pdf = {key : Extract_pdf(crossing_step[key],50) for key in KEYS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(12,3))\n",
    "ax = axs[0]\n",
    "for key in crossing_time_pdf :\n",
    "    ax.semilogy(crossing_time_pdf[key][0],crossing_time_pdf[key][1], label = key);\n",
    "#ax.legend();\n",
    "ax.set_xlim([-1,800])\n",
    "ax = axs[1]\n",
    "for key in crossing_step_pdf :\n",
    "    ax.semilogy(crossing_step_pdf[key][0],crossing_step_pdf[key][1], label = key);\n",
    "ax.legend();\n",
    "ax.set_xlim([-1,40000]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of time step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_histfname = ODIR+'dt_hist.dat'\n",
    "dtmeanfname  = ODIR+'dt_mean.dat'\n",
    "dtstdfname   = ODIR+'dt_std.dat'\n",
    "dt_hist = {}; dtDNS_mean = {}; dtDNS_std = {};\n",
    "print('Loading data...')\n",
    "if os.path.isfile(dt_histfname) : dt_hist    = Load_pdf(dt_histfname)\n",
    "if os.path.isfile(dtmeanfname)  : dtDNS_mean = Load_data(dtmeanfname)\n",
    "if os.path.isfile(dtstdfname)   : dtDNS_std  = Load_data(dtstdfname)\n",
    "print('Done.')\n",
    "\n",
    "RESAVE = False\n",
    "for key in tDNS :\n",
    "    if (key not in dt_hist) or (key not in dtDNS_mean) or (key not in dtDNS_std) or RERUN:\n",
    "        RESAVE = True\n",
    "        print(\"missing \" + key + \" data..\")        \n",
    "        k_,h_,m_,s_ = time_step_analysis(tDNS[key],key)\n",
    "        dt_hist[key]    = h_.tolist()\n",
    "        dtDNS_mean[key] = [m_]\n",
    "        dtDNS_std[key]  = [s_]\n",
    "        \n",
    "if RESAVE :\n",
    "    print('Saving '+dt_histfname+'...')\n",
    "    Save_pdf(dt_hist,dt_histfname)\n",
    "    print('Saving '+dtmeanfname+'...')\n",
    "    Save_data(dtDNS_mean,dtmeanfname)\n",
    "    print('Saving '+dtstdfname+'...')\n",
    "    Save_data(dtDNS_std,dtstdfname)\n",
    "    print('Done.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1,figsize=(5,4))\n",
    "ax = axs\n",
    "k_ = 0\n",
    "for key in KEYS:\n",
    "    ax.plot(dt_hist[key][0],dt_hist[key][1], label = key, color = color_cycle[k_])\n",
    "    ax.plot(np.ones(2) * dtDNS_mean[key],np.linspace(0,500,2),'--', color = color_cycle[k_])\n",
    "    k_ += 1\n",
    "ax.set_title('Distribution of time step');\n",
    "ax.set_xlabel('dt')\n",
    "ax.set_ylabel('p(dt)')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1,figsize=(5,4))\n",
    "ax = axs\n",
    "ax.set_title('Mean dt value')\n",
    "ax.plot(PECLET[1:],[dtDNS_mean[k_][0]/PARAM[k_]['CSF'] for k_ in KEYS[1:]],'ob')\n",
    "ax.plot([0.1, 1000],[dtDNS_mean[KEYS[0]] for k_ in [0.1, 1000]],'--b', alpha = 0.5, label = 'Pe = '+v_to_pow(PECLET[0]))\n",
    "x_ = np.linspace(0.1,100, 10)\n",
    "y_ = 2*x_*(3e-6/UREF/TREF)**2\n",
    "ax.plot(x_,y_,'-.k', alpha = 0.4, label = r'$dt_D(\\mathrm{Pe})$')\n",
    "ax.plot()\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(r'Pe')\n",
    "ax.set_ylabel(r'$\\langle dt \\rangle$')\n",
    "ax.legend()\n",
    "plt.savefig(FDIR+'DNS_dt_distribution'+extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making DNS data isochronous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isochronous transform of DNS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single stream line transform\n",
    "def ssl_transform(N,tDNS,vDNS) :\n",
    "    t_ssl   = []\n",
    "    v_ssl   = []\n",
    "    told    = 0.0\n",
    "    T       = 0.0\n",
    "    for  (t_,v_) in zip(tDNS,vDNS) :\n",
    "        if t_ >= told :\n",
    "            T += t_ - told\n",
    "            t_ssl.append(T)\n",
    "            v_ssl.append(v_)\n",
    "        told = t_\n",
    "    return np.array(t_ssl), np.array(v_ssl)\n",
    "\n",
    "#pointwise linear interpolation\n",
    "def lin_interp(v1,t1,v0,t0,t) :\n",
    "    v = v0 + (v1-v0) * (t-t0)/(t1-t0)\n",
    "    return v\n",
    "\n",
    "#Alternative version (parallel over dict keys) :\n",
    "def isochronous_transform(t_, v_, dt, key, Npoints, ssltrans = True) :\n",
    "    if ssltrans : t_serie, v_serie = ssl_transform (len(t_),t_, v_)\n",
    "    if Npoints == 'all' :\n",
    "        N = len(t_serie)-1\n",
    "    elif 0 < Npoints <= 1 :\n",
    "        N = int(Npoints * (len(t_serie)-1))\n",
    "    else :\n",
    "        N = Npoints\n",
    "    Nt_iso   = int(np.ceil(t_serie[N]/dt))\n",
    "    v_iso    = np.zeros(Nt_iso)\n",
    "    \n",
    "    m = 0; i = 0; ti = 0;\n",
    "    while(m < N-1) :\n",
    "        t1 = t_serie[m+1]; t0 = t_serie[m]\n",
    "        v1 = v_serie[m+1]; v0 = v_serie[m]\n",
    "        while (t0 <= ti <= t1) :\n",
    "            v_iso[i]  = lin_interp(v1,t1,v0,t0,ti)\n",
    "            ti += dt; i += 1\n",
    "        m = m + 1\n",
    "    return (key,v_iso)\n",
    "\n",
    "def parallel_isochronous_transform(t_dict, v_dict, dt_dict, Npoints, dt_factor, MISSING_KEYS) :\n",
    "    v_iso    = Parallel(n_jobs=NCORES_MAX)(\n",
    "        delayed(isochronous_transform)\n",
    "        (t_dict[key], v_dict[key], dt_dict[key][0] * dt_factor, key, Npoints) for key in MISSING_KEYS)\n",
    "    v_iso_dict = {}\n",
    "    for (key,v_) in v_iso :\n",
    "        v_iso_dict[key] = v_.tolist()\n",
    "    return v_iso_dict\n",
    "\n",
    "def sequential_isochronous_transform(t_dict, v_dict, dt_dict, Npoints, dt_factor) :\n",
    "    v_iso = [isochronous_transform(t_dict[key], v_dict[key], dt_dict[key] * dt_factor, key, Npoints) for key in KEYS]\n",
    "    v_iso_dict = {}\n",
    "    for (key,v_) in v_iso :\n",
    "        v_iso_dict[key] = v_.tolist()\n",
    "    return v_iso_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionnary to store isochronous transforms with various dt\n",
    "dt_factor = 1.0 #To choose the ratio between isochr dt and DNS mean dt\n",
    "v_iso     = {}; \n",
    "theta_iso = {}\n",
    "t_iso     = {} #time (linspace(0,T,dt))\n",
    "Nt_iso    = {} #number of points\n",
    "dt_iso    = {}; \n",
    "Npoints   = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Velocity isochronous transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vname     = ODIR+'v_iso.dat'\n",
    "if  os.path.isfile(vname) and (not RERUN):\n",
    "    print('Loading v_iso.dat...')\n",
    "    v_iso = Load_data(vname)\n",
    "\n",
    "RESAVE_V     = False\n",
    "iso_missing = {}\n",
    "MISSING_KEYS = []\n",
    "for key in KEYS : \n",
    "    if key not in v_iso :\n",
    "        RESAVE_V = True\n",
    "        print('Missing '+key+' in v_iso..')\n",
    "        MISSING_KEYS.append(key)\n",
    "\n",
    "if MISSING_KEYS :\n",
    "    start = time.time()\n",
    "    iso_missing  = parallel_isochronous_transform(tDNS, vDNS, dtDNS_mean, Npoints, dt_factor, MISSING_KEYS)\n",
    "    for key in iso_missing :\n",
    "        v_iso[key] = iso_missing[key]\n",
    "    end = time.time()\n",
    "    print('\\tTime needed \\t: '+str(end-start))\n",
    "\n",
    "if RESAVE_V :\n",
    "    print('Saving '+vname+'...')\n",
    "    tosave = {k_ : v_iso[k_] for k_ in KEYS}\n",
    "    Save_data(tosave,vname)\n",
    "tosave = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtname    = ODIR+'dt_iso.dat'\n",
    "if  os.path.isfile(dtname) and (not RERUN) :\n",
    "    print('Loading dt_iso.dat...')\n",
    "    dt_iso = Load_data(dtname)\n",
    "\n",
    "for key in KEYS :\n",
    "    print(key+' :')\n",
    "    Nt_iso[key]  = len(v_iso[key])\n",
    "    dt  = dtDNS_mean[key][0]\n",
    "    dt_iso[key] = [dt]\n",
    "\n",
    "Save_data(dt_iso,dtname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theta equidistant transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_transform_2(dlDNS, tDNS, vDNS ):\n",
    "    l_ = 0\n",
    "    v_ = vDNS[0]\n",
    "    lssl = [l_]\n",
    "    vssl = [v_]\n",
    "    for i in range(len(dlDNS)-1) :\n",
    "        if tDNS[i+1] > tDNS[i] :\n",
    "            l_ += dlDNS[i]\n",
    "            v_  = vDNS[i]\n",
    "            lssl.append(l_)\n",
    "            vssl.append(v_)\n",
    "    dl = np.mean(np.array(lssl[1:])-np.array(lssl[:-1]))\n",
    "    return lssl, vssl, dl\n",
    "\n",
    "def equidistant_transform_2 (dlDNS, tDNS, vDNS, key):\n",
    "    l, v, dl = ssl_transform_2(dlDNS, tDNS, vDNS)\n",
    "    N     = len(l)\n",
    "    N_eq_ = int(np.ceil(l[-1]/dl))\n",
    "    l_eq_ = 0\n",
    "    v_eq_ = np.zeros(N_eq_)\n",
    "    i = 0; j = 0;\n",
    "    while j < N-1 :\n",
    "        v1 = v[j+1]; v0 = v[j]\n",
    "        l1 = l[j+1]; l0 = l[j]\n",
    "        while l0 <= l_eq_ <= l1 and i < N_eq_ :\n",
    "            v_eq_[i] = v0 + (v1-v0) * (l_eq_-l0)/(l1-l0)\n",
    "            l_eq_ += dl\n",
    "            i += 1\n",
    "        j += 1\n",
    "    return v, v_eq_, dl, key\n",
    "\n",
    "def equidistant_transform (l, v, dl, key):\n",
    "    #l, v, dl = ssl_transform_2(dlDNS, tDNS, vDNS)\n",
    "    N     = len(l)\n",
    "    N_eq_ = int(np.ceil(l[-1]/dl))\n",
    "    l_eq_ = 0\n",
    "    v_eq_ = np.zeros(N_eq_)\n",
    "    i = 0; j = 0;\n",
    "    while j < N-1 :\n",
    "        v1 = v[j+1]; v0 = v[j]\n",
    "        l1 = l[j+1]; l0 = l[j]\n",
    "        while l0 <= l_eq_ <= l1 and i < N_eq_ :\n",
    "            v_eq_[i] = v0 + (v1-v0) * (l_eq_-l0)/(l1-l0)\n",
    "            l_eq_ += dl\n",
    "            i += 1\n",
    "        j += 1\n",
    "    return v_eq_\n",
    "\n",
    "def parallel_equidistant_transform(dlDNS_dict, tDNS_dict, vDNS_dict, MISSING_KEYS) :\n",
    "    results = Parallel(n_jobs=NCORES_MAX)(\n",
    "        delayed(equidistant_transform_2)\n",
    "        (dlDNS_dict[key], tDNS_dict[key], vDNS_dict[key], key) for key in MISSING_KEYS)\n",
    "    vssl_dict = {}\n",
    "    v_eq_dict = {}\n",
    "    dl_eq_dict = {}\n",
    "    for (vssl_, v_eq_, dl_, key_) in results :\n",
    "        vssl_dict[key]  = vssl_\n",
    "        v_eq_dict[key] = v_eq_.tolist()\n",
    "        dl_eq_dict[key] = dl_\n",
    "    return vssl_dict, v_eq_dict, dl_eq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lssl  = {}; theta_ssl = {}\n",
    "dl_eq = {}; theta_eq = {} \n",
    "\n",
    "theta_eqname= ODIR+'theta_eq.dat'\n",
    "if  os.path.isfile(theta_eqname) and (not RERUN) :\n",
    "    theta_eq = Load_data(theta_eqname)\n",
    "    \n",
    "dl_eqname= ODIR+'dl_eq.dat'\n",
    "if  os.path.isfile(dl_eqname) and (not RERUN) :\n",
    "    dl_eq = Load_data(dl_eqname)\n",
    "    \n",
    "MISSING_KEYS = []\n",
    "for key in KEYS[:1] :\n",
    "    if key not in theta_eq :\n",
    "        MISSING_KEYS.append(key)\n",
    "\n",
    "theta_ssl, theta_eq, dl_eq = parallel_equidistant_transform (dlDNS, tDNS, thetaDNS, MISSING_KEYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving '+theta_eqname+'...')\n",
    "Save_data(theta_eq,theta_eqname)\n",
    "\n",
    "print('Saving '+dl_eqname+'...')\n",
    "tosave = {k_ :[dl_eq[k_]] for k_ in dl_eq}\n",
    "Save_data(tosave,dl_eqname)\n",
    "tosave = {}\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dl_eq[KEYS[0]]*len(theta_eq[KEYS[0]]))\n",
    "print(np.sum(dlDNS[KEYS[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = KEYS[0]\n",
    "N0  = 100000\n",
    "Np  = 100000\n",
    "plt.plot([i*dl_eq[key] for i in range(N0,N0+Np)],theta_eq[key][N0:N0+Np],'-');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(theta_eq[key]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification of the isochronous transform by comparing it to original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key    = KEYS[-1]\n",
    "fig, axs = plt.subplots(1,3, figsize = (8,3.0), sharey = True)\n",
    "i0  = 50\n",
    "N   = 100\n",
    "for (i,key) in zip(range(3),KEYS) :\n",
    "    T0     = tDNS[key][i0]\n",
    "    T      = tDNS[key][i0+N]\n",
    "    axs[i].set_title('Pe = '+v_to_pow(PECLET[i]))\n",
    "    axs[i].plot(tDNS[key][i0:i0+N],vDNS[key][i0:i0+N],'.', label = 'DNS series'); #rough DNS series\n",
    "    \n",
    "    i0_iso = int(np.ceil(T0/dt_iso[key]))\n",
    "    N_iso  = int(np.ceil((T-T0)/dt_iso[key]))\n",
    "    t_iso  = np.linspace(T0,T,N_iso)\n",
    "    axs[i].plot(t_iso,v_iso[key][i0_iso:i0_iso+N_iso],'-',label='isochronous series'); #Isochronous series\n",
    "\n",
    "    axs[i].set_xlabel('t/T');\n",
    "    axs[0].set_ylabel('v');\n",
    "axs[i].legend();\n",
    "plt.tight_layout()\n",
    "plt.savefig(FDIR+'check_iso_transf'+extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(15,5), sharey = True);\n",
    "axs[0].set_title('Maximal physical time');\n",
    "axs[0].semilogy(PECLET,[tDNS[k_][-1] for k_ in KEYS],'--o');\n",
    "axs[0].set_xlabel(r'$D_m$');\n",
    "axs[0].set_ylabel(r'$T_{max}$');\n",
    "\n",
    "axs[1].set_title('Number of sampling points');\n",
    "axs[1].loglog(PECLET,[len(tDNS[k_]) for k_ in KEYS],'--o');\n",
    "axs[1].set_xlabel(r'$D_m$');\n",
    "axs[1].set_ylabel(r'$N$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Markovian processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [Bentheimer1000]\n",
    "Nmp = 100\n",
    "MP_params = {\n",
    "'seed': None,\n",
    "'v0'  : 1.5,\n",
    "'dt'  : 0.005,\n",
    "'Tmax': 300.0,\n",
    "'csf' : 1.0\n",
    "}\n",
    "MP_lvm  = {'params' : MP_params}\n",
    "fname   = DIR+'Markov_processes/MP_lvm.dat'\n",
    "rerun   = False\n",
    "if (not os.path.isfile(fname)) or rerun:\n",
    "    for sample in samples :\n",
    "        MP_lvm[sample['name']]  = Parallel(n_jobs=NCORES_MAX)(\n",
    "            delayed(LVM_process)\n",
    "            (sample, MP_params) for i in range(Nmp))\n",
    "        MP_lvm[sample['name']]=np.reshape(MP_lvm[sample['name']],len(MP_lvm[sample['name']][0])*Nmp)\n",
    "        Save_data(MP_lvm,fname)\n",
    "else :\n",
    "    print('Loading data...')\n",
    "    MP_lvm = Load_data(fname)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation of LVM PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_vDNS           = {} #Log velocity magnitude probability density function extracted directly from DNS\n",
    "pdf_vDNS_fname     = ODIR+'pdf_vDNS.dat'\n",
    "pdf_viso           = {} #to store isochronous histogram as a line\n",
    "pdf_viso_fname     = ODIR+'pdf_viso.dat'\n",
    "\n",
    "pdf_thetaDNS       = {} #Directional angle probability density function extracted directly from DNS\n",
    "pdf_thetaDNS_fname = ODIR+'pdf_thetaDNS.dat'\n",
    "pdf_thetaiso       = {} \n",
    "pdf_thetaiso_fname = ODIR+'pdf_thetaiso.dat'\n",
    "pdf_thetaeq        = {} \n",
    "pdf_thetaeq_fname  = ODIR+'pdf_thetaeq.dat'\n",
    "pdf_thetaeq_abs   = {} \n",
    "pdf_thetaeq_abs_fname  = ODIR+'pdf_thetaeq_abs.dat'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(pdf_vDNS_fname): \n",
    "    print('Loading vDNS pdf..')\n",
    "    pdf_vDNS = Load_pdf(pdf_vDNS_fname)\n",
    "    \n",
    "if os.path.isfile(pdf_viso_fname): \n",
    "    print('Loading viso pdf..')\n",
    "    pdf_viso = Load_pdf(pdf_viso_fname)\n",
    "    \n",
    "if os.path.isfile(pdf_thetaDNS_fname): \n",
    "    print('Loading thetaDNS pdf..')\n",
    "    pdf_thetaDNS = Load_pdf(pdf_thetaDNS_fname)\n",
    "    \n",
    "if os.path.isfile(pdf_thetaiso_fname): \n",
    "    print('Loading thetaiso pdf..')\n",
    "    pdf_thetaiso = Load_pdf(pdf_thetaiso_fname)\n",
    "\n",
    "if os.path.isfile(pdf_thetaeq_fname): \n",
    "    print('Loading thetaeq pdf..')\n",
    "    pdf_thetaeq = Load_pdf(pdf_thetaeq_fname)\n",
    "    \n",
    "if os.path.isfile(pdf_thetaeq_abs_fname):\n",
    "    print('Loading thetaeq_abs pdf..')\n",
    "    pdf_thetaeq_abs = Load_pdf(pdf_thetaeq_abs_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = False\n",
    "for key in KEYS :\n",
    "    if key not in pdf_vDNS :\n",
    "        SAVE= True\n",
    "        print(\"missing \" + key + \" data..\")        \n",
    "        pdf_vDNS[key] = Extract_pdf(vDNS[key])\n",
    "        pdf_viso[key] = Extract_pdf(v_iso[key])\n",
    "    if key not in pdf_thetaDNS and key == KEYS[0] :\n",
    "        SAVE= True\n",
    "        print(\"missing \" + key + \" data in pdf_thetaDNS..\")        \n",
    "        pdf_thetaDNS[key] = Extract_pdf(np.abs([tt_ for tt_ in thetaDNS[key] if str(tt_) != 'nan']))\n",
    "        \n",
    "    if key not in pdf_thetaiso and key == KEYS[0] :\n",
    "        SAVE= True\n",
    "        print(\"missing \" + key + \" data in pdf_thetaiso..\")        \n",
    "        pdf_thetaiso[key] = Extract_pdf(np.abs([tt_ for tt_ in theta_iso[key]if str(tt_) != 'nan']))\n",
    "\n",
    "    if key not in pdf_thetaeq and key == KEYS[0] :\n",
    "        SAVE= True\n",
    "        print(\"missing \" + key + \" data ub pdf_thetaeq..\")        \n",
    "        pdf_thetaeq[key] = Extract_pdf(theta_eq[key])\n",
    "\n",
    "    if key not in pdf_thetaeq_abs and key == KEYS[0] :\n",
    "        SAVE= True\n",
    "        print(\"missing \" + key + \" data in pdf_thetaeq abs..\")        \n",
    "        pdf_thetaeq_abs[key] = Extract_pdf(np.abs([tt_ for tt_ in theta_eq[key]if str(tt_) != 'nan']))\n",
    "\n",
    "\n",
    "\n",
    "if SAVE :     \n",
    "    print('\\tSaving..')\n",
    "    Save_pdf(pdf_vDNS, pdf_vDNS_fname)\n",
    "    Save_pdf(pdf_viso, pdf_viso_fname)\n",
    "    Save_pdf(pdf_thetaDNS, pdf_thetaDNS_fname)\n",
    "    Save_pdf(pdf_thetaiso, pdf_thetaiso_fname)\n",
    "    Save_pdf(pdf_thetaeq, pdf_thetaeq_fname)\n",
    "    Save_pdf(pdf_thetaeq_abs, pdf_thetaeq_abs_fname)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pdf_thetaeq[KEYS[0]][0],pdf_thetaeq[KEYS[0]][1]);\n",
    "plt.plot(pdf_thetaeq_abs[KEYS[0]][0],pdf_thetaeq_abs[KEYS[0]][1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvmpdf = Load_pdf(ODIR+'bentheimer1000lvmpdf.dat')\n",
    "fig, ax = plt.subplots(1,1,figsize = (5,3.0), sharex=True, sharey=True)\n",
    "sample   = Bentheimer1000\n",
    "vrange = np.linspace(-10,10,100)\n",
    "count    = 0\n",
    "for key in [PLOT_KEYS[0]]:\n",
    "    data1 = vDNS[key]\n",
    "    data2 = v_iso[key]\n",
    "    data3 = MP_lvm[sample['name']]\n",
    "    ax.set_title(r'Pe =$\\infty$')\n",
    "    ax.hist(data3, bins = 200, label = 'Markov process',   density = True, alpha= 0.5, color = color_cycle[3])\n",
    "    ax.hist(data1, bins = 200, label ='DNS',         density = True, alpha= 0.5, color = color_cycle[0])\n",
    "    ax.hist(data2, bins = 200, label ='Isochronous', density = True, alpha= 0.5, color = color_cycle[1])\n",
    "    ax.plot(vrange, p_skew(vrange,sample), label = 'Skew normal PDF', color = 'r');\n",
    "    ax.plot(lvmpdf['Bentheimer1000'][0], lvmpdf['Bentheimer1000'][1], '--k', label = 'Eulerian', alpha=0.7);\n",
    "    #ax.grid(True)\n",
    "    ax.set_xlabel('v')\n",
    "    ax.set_ylabel('p(v)')\n",
    "    ax.legend()\n",
    "    ax.set_xlim([-8,4])\n",
    "plt.tight_layout()\n",
    "plt.savefig(FDIR+'LVM_pdf_DM=0'+extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvmpdf = Load_pdf(ODIR+'bentheimer1000lvmpdf.dat')\n",
    "#fig, axs = plt.subplots(int(len(PLOT_KEYS)/2),2,figsize=(8,3*(int(len(PLOT_KEYS)/2))+1), sharex=True, sharey=True)\n",
    "fig, axs = plt.subplots(len(PLOT_KEYS),1,figsize=(5,3*int(len(PLOT_KEYS))), sharex=True, sharey=True)\n",
    "sample   = Bentheimer1000\n",
    "vrange = np.linspace(-10,10,100)\n",
    "count    = 0\n",
    "for key in PLOT_KEYS:\n",
    "    #(i,j) = (int(count/2),np.mod(count,2))\n",
    "    #ax = axs[i,j]\n",
    "    ax = axs[count]\n",
    "    ax.set_title(key)#'Pe='+v_to_pow(PECLET[count]))\n",
    "    ax.hist(vDNS[key], bins = 200, label = 'DNS',         density = True, alpha= 0.5, color = color_cycle[0])\n",
    "    ax.hist(v_iso[key], bins = 200, label = 'Isochronous', density = True, alpha= 0.5, color = color_cycle[1])\n",
    "    ax.plot(vrange, p_skew(vrange,sample), label = 'Skew normal PDF', color = 'r');\n",
    "    ax.plot(lvmpdf['Bentheimer1000'][0], lvmpdf['Bentheimer1000'][1], '--k', label = 'Eulerian', alpha=0.7);\n",
    "    #ax.grid(True)\n",
    "    ax.set_xlabel('v')\n",
    "    ax.set_ylabel('p(v)')\n",
    "    ax.legend()\n",
    "    ax.set_xlim([-8,4])\n",
    "    count += 1\n",
    "plt.tight_layout()\n",
    "plt.savefig(FDIR+'LVM_pdf_comparison'+extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
